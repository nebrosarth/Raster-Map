{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, concatenate, Activation, add, \\\n",
    "    multiply, Lambda, Dropout, BatchNormalization, UpSampling2D\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T05:49:09.006081Z",
     "start_time": "2024-03-13T05:49:08.995551Z"
    }
   },
   "id": "d97877819d6b421c",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "kinit = 'he_normal'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T05:49:09.036099Z",
     "start_time": "2024-03-13T05:49:09.007753Z"
    }
   },
   "id": "c520f2f2f38ca6f6",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def AttnGatingBlock(x, g, inter_shape, name):\n",
    "    ''' take g which is the spatially smaller signal, do a conv to get the same\n",
    "    number of feature channels as x (bigger spatially)\n",
    "    do a conv on x to also get same geature channels (theta_x)\n",
    "    then, upsample g to be same size as x \n",
    "    add x and g (concat_xg)\n",
    "    relu, 1x1 conv, then sigmoid then upsample the final - this gives us attn coefficients'''\n",
    "\n",
    "    shape_x = K.int_shape(x)  # 32\n",
    "    shape_g = K.int_shape(g)  # 16\n",
    "\n",
    "    theta_x = Conv2D(inter_shape, (2, 2), strides=(2, 2), padding='same', name='xl' + name)(x)  # 16\n",
    "    shape_theta_x = K.int_shape(theta_x)\n",
    "\n",
    "    phi_g = Conv2D(inter_shape, (1, 1), padding='same')(g)\n",
    "    upsample_g = Conv2DTranspose(inter_shape, (3, 3),\n",
    "                                 strides=(shape_theta_x[1] // shape_g[1], shape_theta_x[2] // shape_g[2]),\n",
    "                                 padding='same', name='g_up' + name)(phi_g)  # 16\n",
    "\n",
    "    concat_xg = add([upsample_g, theta_x])\n",
    "    act_xg = Activation('relu')(concat_xg)\n",
    "    psi = Conv2D(1, (1, 1), padding='same', name='psi' + name)(act_xg)\n",
    "    sigmoid_xg = Activation('sigmoid')(psi)\n",
    "    shape_sigmoid = K.int_shape(sigmoid_xg)\n",
    "    upsample_psi = UpSampling2D(size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2]))(sigmoid_xg)  # 32\n",
    "\n",
    "    upsample_psi = expend_as(upsample_psi, shape_x[3], name)\n",
    "    y = multiply([upsample_psi, x], name='q_attn' + name)\n",
    "\n",
    "    result = Conv2D(shape_x[3], (1, 1), padding='same', name='q_attn_conv' + name)(y)\n",
    "    result_bn = BatchNormalization(name='q_attn_bn' + name)(result)\n",
    "    return result_bn\n",
    "\n",
    "\n",
    "def UnetGatingSignal(input, is_batchnorm, name):\n",
    "    ''' this is simply 1x1 convolution, bn, activation '''\n",
    "    shape = K.int_shape(input)\n",
    "    x = Conv2D(shape[3] * 1, (1, 1), strides=(1, 1), padding=\"same\", name=name + '_conv')(input)\n",
    "    if is_batchnorm:\n",
    "        x = BatchNormalization(name=name + '_bn')(x)\n",
    "    x = Activation('relu', name=name + '_act')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def UnetConv2D(input, outdim, is_batchnorm, name):\n",
    "    x = Conv2D(outdim, (3, 3), strides=(1, 1), kernel_initializer=\"he_normal\", padding=\"same\", name=name + '_1')(input)\n",
    "    if is_batchnorm:\n",
    "        x = BatchNormalization(name=name + '_1_bn')(x)\n",
    "    x = Activation('relu', name=name + '_1_act')(x)\n",
    "\n",
    "    x = Conv2D(outdim, (3, 3), strides=(1, 1), kernel_initializer=\"he_normal\", padding=\"same\", name=name + '_2')(x)\n",
    "    if is_batchnorm:\n",
    "        x = BatchNormalization(name=name + '_2_bn')(x)\n",
    "    x = Activation('relu', name=name + '_2_act')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def expend_as(tensor, rep, name):\n",
    "    my_repeat = Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=3), arguments={'repnum': rep},\n",
    "                       name='psi_up' + name)(tensor)\n",
    "    return my_repeat"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T05:49:09.051785Z",
     "start_time": "2024-03-13T05:49:09.036099Z"
    }
   },
   "id": "29558067a4ed580",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def dsc(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    score = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return score\n",
    "\n",
    "\n",
    "def tp(y_true, y_pred):\n",
    "    smooth = 1\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    tp = (K.sum(y_pos * y_pred_pos) + smooth) / (K.sum(y_pos) + smooth)\n",
    "    return tp\n",
    "\n",
    "\n",
    "def tn(y_true, y_pred):\n",
    "    smooth = 1\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos\n",
    "    tn = (K.sum(y_neg * y_pred_neg) + smooth) / (K.sum(y_neg) + smooth)\n",
    "    return tn\n",
    "\n",
    "def confusion(y_true, y_pred):\n",
    "    smooth = 1\n",
    "    y_pred_pos = K.clip(y_pred, 0, 1)\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "    y_pos = K.clip(y_true, 0, 1)\n",
    "    y_neg = 1 - y_pos\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    "    prec = (tp + smooth) / (tp + fp + smooth)\n",
    "    recall = (tp + smooth) / (tp + fn + smooth)\n",
    "    return prec, recall\n",
    "\n",
    "def prec(y_true, y_pred):\n",
    "    _, recall = confusion(y_true, y_pred)\n",
    "    return recall\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    prec, _ = confusion(y_true, y_pred)\n",
    "    return prec"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T05:49:09.082897Z",
     "start_time": "2024-03-13T05:49:09.053909Z"
    }
   },
   "id": "99aea33c1036d973",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-13T05:49:09.098462Z",
     "start_time": "2024-03-13T05:49:09.084005Z"
    }
   },
   "outputs": [],
   "source": [
    "def attn_unet(opt, input_size, lossfxn):\n",
    "    inputs = Input(shape=input_size)\n",
    "    conv1 = UnetConv2D(inputs, 32, is_batchnorm=True, name='conv1')\n",
    "    conv1 = Dropout(0.2, name='drop_conv1')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = UnetConv2D(pool1, 32, is_batchnorm=True, name='conv2')\n",
    "    conv2 = Dropout(0.2, name='drop_conv2')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = UnetConv2D(pool2, 64, is_batchnorm=True, name='conv3')\n",
    "    conv3 = Dropout(0.2, name='drop_conv3')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = UnetConv2D(pool3, 64, is_batchnorm=True, name='conv4')\n",
    "    conv4 = Dropout(0.2, name='drop_conv4')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    center = UnetConv2D(pool4, 128, is_batchnorm=True, name='center')\n",
    "\n",
    "    g1 = UnetGatingSignal(center, is_batchnorm=True, name='g1')\n",
    "    attn1 = AttnGatingBlock(conv4, g1, 128, '_1')\n",
    "    up1 = concatenate([Conv2DTranspose(32, (3, 3), strides=(2, 2), padding='same', activation='relu',\n",
    "                                       kernel_initializer=kinit)(center), attn1], name='up1')\n",
    "\n",
    "    g2 = UnetGatingSignal(up1, is_batchnorm=True, name='g2')\n",
    "    attn2 = AttnGatingBlock(conv3, g2, 64, '_2')\n",
    "    up2 = concatenate(\n",
    "        [Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same', activation='relu', kernel_initializer=kinit)(up1),\n",
    "         attn2], name='up2')\n",
    "\n",
    "    g3 = UnetGatingSignal(up1, is_batchnorm=True, name='g3')\n",
    "    attn3 = AttnGatingBlock(conv2, g3, 32, '_3')\n",
    "    up3 = concatenate(\n",
    "        [Conv2DTranspose(32, (3, 3), strides=(2, 2), padding='same', activation='relu', kernel_initializer=kinit)(up2),\n",
    "         attn3], name='up3')\n",
    "\n",
    "    up4 = concatenate(\n",
    "        [Conv2DTranspose(32, (3, 3), strides=(2, 2), padding='same', activation='relu', kernel_initializer=kinit)(up3),\n",
    "         conv1], name='up4')\n",
    "    out = Conv2D(1, (1, 1), activation='sigmoid', kernel_initializer=kinit, name='final')(up4)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[out])\n",
    "    model.compile(optimizer=opt, loss=lossfxn, metrics=[dsc, tp, tn, prec, recall])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = attn_unet('adam', (256, 256, 1), 'binary_crossentropy')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T05:49:16.592206Z",
     "start_time": "2024-03-13T05:49:16.146546Z"
    }
   },
   "id": "a1091bb30e106aaf",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer count mismatch when loading weights from file. Model expected 49 layers, found 30 saved layers.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_weights\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m../model/model-T_unet-maproad-fc2d.weights.h5\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32mD:\\Python310\\lib\\site-packages\\keras\\saving\\hdf5_format.py:817\u001B[0m, in \u001B[0;36mload_weights_from_hdf5_group\u001B[1;34m(f, model)\u001B[0m\n\u001B[0;32m    815\u001B[0m layer_names \u001B[38;5;241m=\u001B[39m filtered_layer_names\n\u001B[0;32m    816\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(layer_names) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(filtered_layers):\n\u001B[1;32m--> 817\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    818\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLayer count mismatch when loading weights from file. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    819\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel expected \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(filtered_layers)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m layers, found \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    820\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(layer_names)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m saved layers.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    821\u001B[0m     )\n\u001B[0;32m    823\u001B[0m \u001B[38;5;66;03m# We batch weight value assignments in a single backend call\u001B[39;00m\n\u001B[0;32m    824\u001B[0m \u001B[38;5;66;03m# which provides a speedup in TensorFlow.\u001B[39;00m\n\u001B[0;32m    825\u001B[0m weight_value_tuples \u001B[38;5;241m=\u001B[39m []\n",
      "\u001B[1;31mValueError\u001B[0m: Layer count mismatch when loading weights from file. Model expected 49 layers, found 30 saved layers."
     ]
    }
   ],
   "source": [
    "model.load_weights('../model/model-T_unet-maproad-fc2d.weights.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-13T05:53:33.549955Z",
     "start_time": "2024-03-13T05:53:33.501322Z"
    }
   },
   "id": "74a41d75d3328831",
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
